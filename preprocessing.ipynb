{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust these directories to point to your video and ground truth files.\n",
    "# video_directory = r\"C:\\Users\\johna\\Desktop\\CS3264proj\\testing\\vid\"  \n",
    "# label_directory = r\"C:\\Users\\johna\\Desktop\\CS3264proj\\testing\\label\"     \n",
    "# video_directory1 = r\"C:\\Users\\johna\\Desktop\\CS3264proj\\Coffee_room_01\\Coffee_room_01\\Videos\"\n",
    "# video_directory2 = r\"C:\\Users\\johna\\Desktop\\CS3264proj\\Coffee_room_02\\Coffee_room_02\\Videos\"\n",
    "video_directory3 = r\"C:\\Users\\johna\\Desktop\\CS3264proj\\Home_01\\Home_01\\Videos\"\n",
    "video_directory4 = r\"C:\\Users\\johna\\Desktop\\CS3264proj\\Home_02\\Home_02\\Videos\"\n",
    "label_directory1 = r\"C:\\Users\\johna\\Desktop\\CS3264proj\\Coffee_room_01\\Coffee_room_01\\Annotation_files\"\n",
    "label_directory2 = r\"C:\\Users\\johna\\Desktop\\CS3264proj\\Coffee_room_02\\Coffee_room_02\\Annotation_files\"\n",
    "label_directory3 = r\"C:\\Users\\johna\\Desktop\\CS3264proj\\Home_01\\Home_01\\Annotation_files\"\n",
    "label_directory4 = r\"C:\\Users\\johna\\Desktop\\CS3264proj\\Home_02\\Home_02\\Annotation_files\"\n",
    "\n",
    "video_directory = [video_directory3, video_directory4]\n",
    "label_directory = [label_directory3, label_directory4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mediapipe pose initialization\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_detector = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "pose_estimator = mp_pose.Pose(static_image_mode=True)\n",
    "\n",
    "# Skeleton edges (Mediapipe format)\n",
    "EDGES = [\n",
    "    (0,1),(1,2),(2,3),(3,7),(0,4),(4,5),(5,6),(6,8),\n",
    "    (9,10),(11,12),(12,14),(14,16),(11,13),(13,15),\n",
    "    (12,24),(24,23),(23,11),(24,26),(26,28),(28,32),\n",
    "    (23,25),(25,27),(27,31)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pose_from_frame(frame):\n",
    "    results = pose_estimator.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) #nered to convert to rgb for mediapipe\n",
    "    if results.pose_landmarks:\n",
    "        pose_data = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in results.pose_landmarks.landmark]) #x,y,z,visibility from mediapipe\n",
    "        return pose_data\n",
    "    return np.zeros((33, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ground_truth(gt_file):\n",
    "    with open(gt_file, 'r') as f:\n",
    "        lines = f.readlines() #since the first 2 line is the start fram of fale and end fram of fall\n",
    "        try:\n",
    "            fall_start = int(lines[0].strip())\n",
    "            fall_end = int(lines[1].strip())\n",
    "        except ValueError:\n",
    "            print(\"This file does not have any fall start and end frame\", gt_file)\n",
    "            fall_start = None\n",
    "            fall_end = None\n",
    "    return fall_start, fall_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(sequence):\n",
    "    T, N, C = sequence.shape #expected for  sample video should be (30,33,4)\n",
    "    x_list = [] #node\n",
    "    edge_index = [] #edges\n",
    "    # Build node features for every joint in every frame.\n",
    "    for t in range(T):\n",
    "        for i in range(N):\n",
    "            x_list.append(sequence[t, i]) \n",
    "    # Add spatial edges within each frame.\n",
    "    # print(x_list)\n",
    "    for t in range(T):\n",
    "        base = t * N\n",
    "        for (src, dst) in EDGES:\n",
    "            edge_index.append([base + src, base + dst])\n",
    "            edge_index.append([base + dst, base + src])\n",
    "    # Add temporal edges by linking between frames (t and t+1).\n",
    "    for t in range(T - 1):\n",
    "        base_current = t * N\n",
    "        base_next = (t + 1) * N\n",
    "        for i in range(N):\n",
    "            edge_index.append([base_current + i, base_next + i])\n",
    "            edge_index.append([base_next + i, base_current + i])\n",
    "    x = torch.tensor(np.array(x_list), dtype=torch.float) #node features\n",
    "    # Convert the edge index list to a tensor.\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    return Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_video(video_path, gt_path, window_size=30, stride=10, max_frames=300):\n",
    "    dataset = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read() #read a frame from video\n",
    "        if not ret or len(frames) >= max_frames: #stop if no frame or max frames reached\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    fall_start, fall_end = parse_ground_truth(gt_path)\n",
    "\n",
    "    for i in range(0, len(frames) - window_size + 1, stride): # iterate over the frames with a stride\n",
    "        window = frames[i:i+window_size] # windows frames act as a sequence\n",
    "        pose_sequence = np.array([extract_pose_from_frame(f) for f in window])\n",
    "        if pose_sequence.shape != (window_size, 33, 4):\n",
    "            continue\n",
    "        graph = build_graph(pose_sequence)\n",
    "        # Label is 1 if any part of the window overlaps the fall event.\n",
    "        if fall_start is None or fall_end is None:\n",
    "            label = 0\n",
    "        else:\n",
    "            # Check if the window overlaps with the fall event.\n",
    "            # The label is 1 if any part of the window overlaps with the fall event.\n",
    "            label = int(not (i+window_size < fall_start or i > fall_end))\n",
    "        graph.y = torch.tensor([label], dtype=torch.long)\n",
    "        dataset.append(graph)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_le2i_dataset(video_dir, label_dir, window_size=30, stride=5):\n",
    "    all_graphs = []\n",
    "    for video_file in sorted(glob.glob(os.path.join(video_dir, '*.avi'))):\n",
    "        base_name = os.path.splitext(os.path.basename(video_file))[0]\n",
    "        gt_file = os.path.join(label_dir, base_name + '.txt')\n",
    "        if not os.path.exists(gt_file):\n",
    "            continue\n",
    "        graphs = create_dataset_from_video(video_file, gt_file, window_size, stride)\n",
    "        all_graphs.extend(graphs)\n",
    "    print(f\"Loaded {len(all_graphs)} graphs from {len(glob.glob(os.path.join(video_dir, '*.avi')))} videos.\")\n",
    "    return all_graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1266 graphs from 30 videos.\n",
      "Dataset 1 loaded with 1266 graphs.\n",
      "Loaded 1276 graphs from 30 videos.\n",
      "Dataset 2 loaded with 1276 graphs.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(video_directory)):\n",
    "    video_dir = video_directory[i]\n",
    "    label_dir = label_directory[i]\n",
    "    dataset = load_le2i_dataset(video_dir, label_dir)\n",
    "    print(f\"Dataset {i+1} loaded with {len(dataset)} graphs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saved dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset \n",
    "torch.save(dataset, 'dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
